#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Embedding æ¨¡å‹æ•ˆæœè¯„ä¼°è„šæœ¬

æœ¬è„šæœ¬ç”¨äºè¯„ä¼°ä¸åŒ Embedding æ¨¡å‹åœ¨ä¸­æ–‡ RAG åœºæ™¯ä¸‹çš„æ•ˆæœï¼š
1. æ£€ç´¢è´¨é‡æŒ‡æ ‡ï¼šHit Rateã€MRRã€NDCGã€Recall@Kã€Precision@K
2. è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼šæ­£è´Ÿæ ·æœ¬åŒºåˆ†åº¦
3. æ¨ç†æ€§èƒ½ï¼šå‘é‡åŒ–é€Ÿåº¦å’Œååé‡
4. å¯è§†åŒ–åˆ†æï¼šç”Ÿæˆå¯¹æ¯”å›¾è¡¨

ä¾èµ–å®‰è£…ï¼š
pip install sentence-transformers numpy matplotlib scikit-learn
"""

import os
import sys
import time
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
import numpy as np


# ============================================================
# é…ç½®å®šä¹‰
# ============================================================
@dataclass
class EmbeddingModelConfig:
    """Embedding æ¨¡å‹é…ç½®"""
    name: str
    model_id: str
    description: str
    max_seq_length: int


# å¾…è¯„ä¼°çš„ Embedding æ¨¡å‹åˆ—è¡¨
EMBEDDING_MODELS = [
    EmbeddingModelConfig(
        name="bge-large-zh",
        model_id="BAAI/bge-large-zh-v1.5",
        description="BGE-large-zh (åŒ—äº¬æ™ºæº, 1024ç»´)",
        max_seq_length=512
    ),
    EmbeddingModelConfig(
        name="bge-base-zh",
        model_id="BAAI/bge-base-zh-v1.5",
        description="BGE-base-zh (åŒ—äº¬æ™ºæº, 768ç»´)",
        max_seq_length=512
    ),
    EmbeddingModelConfig(
        name="text2vec-base",
        model_id="shibing624/text2vec-base-chinese",
        description="text2vec-base-chinese (768ç»´)",
        max_seq_length=128
    ),
    EmbeddingModelConfig(
        name="m3e-base",
        model_id="moka-ai/m3e-base",
        description="M3E-base (Moka AI, 768ç»´)",
        max_seq_length=512
    ),
]


# ============================================================
# æµ‹è¯•æ•°æ®é›†
# ============================================================
def get_evaluation_dataset() -> Dict[str, Any]:
    """
    è·å–è¯„ä¼°æ•°æ®é›†
    
    æ•°æ®é›†åŒ…å«ï¼š
    1. æ–‡æ¡£åº“ï¼šç”¨äºæ„å»ºæ£€ç´¢åº“
    2. æŸ¥è¯¢é›†ï¼šåŒ…å«æŸ¥è¯¢å’Œå¯¹åº”çš„ç›¸å…³æ–‡æ¡£ID
    3. æ­£è´Ÿæ ·æœ¬å¯¹ï¼šç”¨äºæµ‹è¯•è¯­ä¹‰åŒºåˆ†èƒ½åŠ›
    """
    
    # æ–‡æ¡£åº“ï¼ˆæ¨¡æ‹ŸçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ç‰‡æ®µï¼‰
    documents = {
        "doc_1": "RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›ä¸Šä¸‹æ–‡ï¼Œæœ‰æ•ˆå‡å°‘å¹»è§‰é—®é¢˜ã€‚",
        "doc_2": "å‘é‡æ•°æ®åº“æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå¸¸è§çš„æœ‰Chromaã€FAISSã€Milvusã€Pineconeç­‰ã€‚",
        "doc_3": "BGEæ˜¯åŒ—äº¬æ™ºæºç ”ç©¶é™¢å¼€å‘çš„ä¸­æ–‡Embeddingæ¨¡å‹ï¼Œåœ¨è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚",
        "doc_4": "æ–‡æ¡£åˆ†å—ç­–ç•¥ç›´æ¥å½±å“RAGæ£€ç´¢æ•ˆæœï¼Œå¸¸è§ç­–ç•¥åŒ…æ‹¬å›ºå®šå¤§å°åˆ†å—å’Œè¯­ä¹‰åˆ†å—ã€‚",
        "doc_5": "LangChainæ˜¯æ„å»ºRAGåº”ç”¨çš„æµè¡Œæ¡†æ¶ï¼Œæä¾›æ–‡æ¡£åŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–ç­‰å·¥å…·ã€‚",
        "doc_6": "å¤§æ¨¡å‹å¹»è§‰æ˜¯æŒ‡æ¨¡å‹ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…é”™è¯¯çš„å†…å®¹ï¼ŒRAGå¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚",
        "doc_7": "Embeddingæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ã€‚",
        "doc_8": "æ£€ç´¢ç­–ç•¥åŒ…æ‹¬ç›¸ä¼¼åº¦æ£€ç´¢ã€æ··åˆæ£€ç´¢ã€é‡æ’åºç­‰ï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥å¯æå‡æ•ˆæœã€‚",
        "doc_9": "RAGASæ˜¯ä¸€ä¸ªRAGè¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥è‡ªåŠ¨è¯„ä¼°æ£€ç´¢å‡†ç¡®ç‡å’Œå›ç­”è´¨é‡ã€‚",
        "doc_10": "çŸ¥è¯†åº“è´¨é‡ç›´æ¥å½±å“RAGæ•ˆæœï¼Œéœ€è¦ä¿è¯å†…å®¹å‡†ç¡®ã€æ›´æ–°åŠæ—¶ã€è¦†ç›–å…¨é¢ã€‚",
        "doc_11": "Pythonæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸæœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æœºå™¨å­¦ä¹ åº“ã€‚",
        "doc_12": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œç‰¹å¾å­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«ã€‚",
        "doc_13": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„æŠ€æœ¯é¢†åŸŸã€‚",
        "doc_14": "Transformeræ¶æ„æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼Œå¼•å…¥äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚",
        "doc_15": "GPTå’ŒBERTæ˜¯ä¸¤ç§é‡è¦çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¶æ„ï¼Œåˆ†åˆ«é‡‡ç”¨è‡ªå›å½’å’ŒåŒå‘ç¼–ç ã€‚",
    }
    
    # æŸ¥è¯¢é›†ï¼ˆæ¯ä¸ªæŸ¥è¯¢å¯¹åº”çš„ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨ï¼‰
    queries = [
        {
            "query": "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿå®ƒæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
            "relevant_docs": ["doc_1", "doc_6"],
            "category": "åŸºç¡€æ¦‚å¿µ"
        },
        {
            "query": "æœ‰å“ªäº›å¸¸ç”¨çš„å‘é‡æ•°æ®åº“ï¼Ÿ",
            "relevant_docs": ["doc_2"],
            "category": "æŠ€æœ¯ç»„ä»¶"
        },
        {
            "query": "BGEæ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "relevant_docs": ["doc_3", "doc_7"],
            "category": "æ¨¡å‹ç›¸å…³"
        },
        {
            "query": "å¦‚ä½•å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼Ÿ",
            "relevant_docs": ["doc_4"],
            "category": "æŠ€æœ¯ç»†èŠ‚"
        },
        {
            "query": "LangChainæ¡†æ¶çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "relevant_docs": ["doc_5"],
            "category": "å·¥å…·æ¡†æ¶"
        },
        {
            "query": "å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿæ•ˆæœï¼Ÿ",
            "relevant_docs": ["doc_9", "doc_10"],
            "category": "è¯„ä¼°æ–¹æ³•"
        },
        {
            "query": "ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹å¹»è§‰ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ",
            "relevant_docs": ["doc_6", "doc_1"],
            "category": "é—®é¢˜è§£å†³"
        },
        {
            "query": "æ£€ç´¢ç­–ç•¥æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
            "relevant_docs": ["doc_8"],
            "category": "æŠ€æœ¯ç»†èŠ‚"
        },
        {
            "query": "Embeddingæ¨¡å‹çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
            "relevant_docs": ["doc_7", "doc_3"],
            "category": "æ¨¡å‹ç›¸å…³"
        },
        {
            "query": "Transformeræ¶æ„çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "relevant_docs": ["doc_14", "doc_15"],
            "category": "æ·±åº¦å­¦ä¹ "
        },
    ]
    
    # æ­£è´Ÿæ ·æœ¬å¯¹ï¼ˆç”¨äºæµ‹è¯•è¯­ä¹‰åŒºåˆ†èƒ½åŠ›ï¼‰
    # æ ¼å¼ï¼š(æ–‡æœ¬1, æ–‡æœ¬2, æ ‡ç­¾) æ ‡ç­¾1=ç›¸ä¼¼ï¼Œ0=ä¸ç›¸ä¼¼
    semantic_pairs = [
        # æ­£æ ·æœ¬å¯¹ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰
        ("RAGæŠ€æœ¯å¯ä»¥å‡å°‘å¤§æ¨¡å‹å¹»è§‰", "æ£€ç´¢å¢å¼ºç”Ÿæˆæœ‰åŠ©äºæå‡å›ç­”å‡†ç¡®æ€§", 1),
        ("å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æœ¬å‘é‡", "Embeddingå‘é‡è¢«ä¿å­˜åœ¨å‘é‡æ•°æ®åº“ä¸­", 1),
        ("BGEæ˜¯ä¸­æ–‡Embeddingæ¨¡å‹", "åŒ—äº¬æ™ºæºçš„BGEæ¨¡å‹ç”¨äºæ–‡æœ¬å‘é‡åŒ–", 1),
        ("æ–‡æ¡£åˆ†å—å½±å“æ£€ç´¢æ•ˆæœ", "åˆç†çš„åˆ†å—ç­–ç•¥å¯ä»¥æå‡RAGå‡†ç¡®ç‡", 1),
        ("LangChainç”¨äºæ„å»ºRAGåº”ç”¨", "ä½¿ç”¨LangChainæ¡†æ¶å¼€å‘æ£€ç´¢å¢å¼ºç³»ç»Ÿ", 1),
        ("è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIé¢†åŸŸ", "NLPè®©æœºå™¨ç†è§£äººç±»è¯­è¨€", 1),
        ("æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ", "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒ", 1),
        ("Transformerå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶", "è‡ªæ³¨æ„åŠ›æ˜¯Transformerçš„å…³é”®åˆ›æ–°", 1),
        
        # è´Ÿæ ·æœ¬å¯¹ï¼ˆè¯­ä¹‰ä¸ç›¸ä¼¼ï¼‰
        ("RAGæŠ€æœ¯ç”¨äºå¢å¼ºæ£€ç´¢", "ä»Šå¤©å¤©æ°”å¾ˆå¥½é€‚åˆå‡ºé—¨", 0),
        ("å‘é‡æ•°æ®åº“å­˜å‚¨å‘é‡", "æˆ‘å–œæ¬¢åƒè‹¹æœå’Œé¦™è•‰", 0),
        ("BGEæ˜¯Embeddingæ¨¡å‹", "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½åŸå¸‚", 0),
        ("æ–‡æ¡£åˆ†å—ç­–ç•¥å¾ˆé‡è¦", "ç¯®çƒæ˜¯ä¸€é¡¹å›¢é˜Ÿè¿åŠ¨", 0),
        ("LangChainæ˜¯å¼€å‘æ¡†æ¶", "é•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµ", 0),
        ("æœºå™¨å­¦ä¹ æ˜¯AIåˆ†æ”¯", "éŸ³ä¹å¯ä»¥é™¶å†¶æƒ…æ“", 0),
        ("Pythonæ˜¯ç¼–ç¨‹è¯­è¨€", "ç†ŠçŒ«æ˜¯ä¸­å›½å›½å®åŠ¨ç‰©", 0),
        ("Transformeræ¶æ„å¾ˆé‡è¦", "å’–å•¡æœ‰æç¥çš„ä½œç”¨", 0),
    ]
    
    return {
        "documents": documents,
        "queries": queries,
        "semantic_pairs": semantic_pairs
    }


# ============================================================
# è¯„ä¼°æŒ‡æ ‡è®¡ç®—
# ============================================================
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—ç±»"""
    
    @staticmethod
    def hit_rate(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        è®¡ç®— Hit Rate @K
        å¦‚æœTop-Kç»“æœä¸­åŒ…å«ä»»ä¸€ç›¸å…³æ–‡æ¡£ï¼Œåˆ™å‘½ä¸­
        
        Args:
            retrieved_ids: æ£€ç´¢è¿”å›çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k: Top-K
        
        Returns:
            1.0 å¦‚æœå‘½ä¸­ï¼Œå¦åˆ™ 0.0
        """
        top_k = retrieved_ids[:k]
        return 1.0 if any(doc_id in relevant_ids for doc_id in top_k) else 0.0
    
    @staticmethod
    def mrr(retrieved_ids: List[str], relevant_ids: List[str]) -> float:
        """
        è®¡ç®— MRR (Mean Reciprocal Rank)
        ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åçš„å€’æ•°
        
        Args:
            retrieved_ids: æ£€ç´¢è¿”å›çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
        
        Returns:
            1/rank æˆ– 0.0ï¼ˆå¦‚æœæ²¡æœ‰å‘½ä¸­ï¼‰
        """
        for rank, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in relevant_ids:
                return 1.0 / rank
        return 0.0
    
    @staticmethod
    def precision_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        è®¡ç®— Precision @K
        Top-Kç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        
        Args:
            retrieved_ids: æ£€ç´¢è¿”å›çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k: Top-K
        
        Returns:
            ç›¸å…³æ–‡æ¡£æ•° / K
        """
        top_k = retrieved_ids[:k]
        relevant_count = sum(1 for doc_id in top_k if doc_id in relevant_ids)
        return relevant_count / k
    
    @staticmethod
    def recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        è®¡ç®— Recall @K
        Top-Kç»“æœä¸­å¬å›çš„ç›¸å…³æ–‡æ¡£æ¯”ä¾‹
        
        Args:
            retrieved_ids: æ£€ç´¢è¿”å›çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k: Top-K
        
        Returns:
            å¬å›çš„ç›¸å…³æ–‡æ¡£æ•° / æ€»ç›¸å…³æ–‡æ¡£æ•°
        """
        top_k = retrieved_ids[:k]
        recalled = sum(1 for doc_id in top_k if doc_id in relevant_ids)
        return recalled / len(relevant_ids) if relevant_ids else 0.0
    
    @staticmethod
    def ndcg_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
        """
        è®¡ç®— NDCG @K (Normalized Discounted Cumulative Gain)
        è€ƒè™‘æ’åºä½ç½®çš„ç›¸å…³æ€§å¾—åˆ†
        
        Args:
            retrieved_ids: æ£€ç´¢è¿”å›çš„æ–‡æ¡£IDåˆ—è¡¨
            relevant_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
            k: Top-K
        
        Returns:
            NDCG åˆ†æ•°
        """
        def dcg(relevances: List[int]) -> float:
            """è®¡ç®— DCG"""
            return sum(
                rel / np.log2(idx + 2)  # idx+2 å› ä¸º log2(1) = 0
                for idx, rel in enumerate(relevances)
            )
        
        # å®é™…ç›¸å…³æ€§åˆ—è¡¨
        actual_relevances = [
            1 if doc_id in relevant_ids else 0
            for doc_id in retrieved_ids[:k]
        ]
        
        # ç†æƒ³ç›¸å…³æ€§åˆ—è¡¨ï¼ˆæ‰€æœ‰ç›¸å…³æ–‡æ¡£æ’åœ¨å‰é¢ï¼‰
        ideal_relevances = sorted(actual_relevances, reverse=True)
        
        actual_dcg = dcg(actual_relevances)
        ideal_dcg = dcg(ideal_relevances)
        
        return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0
    
    @staticmethod
    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0


# ============================================================
# Embedding æ¨¡å‹è¯„ä¼°å™¨
# ============================================================
class EmbeddingEvaluator:
    """Embedding æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, persist_directory: str = "./chroma_eval_db"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            persist_directory: ChromaDB æŒä¹…åŒ–ç›®å½•
        """
        self.persist_directory = persist_directory
        self.models: Dict[str, Any] = {}
        self.results: Dict[str, Dict] = {}
        self.metrics = EvaluationMetrics()
        
    def load_model(self, config: EmbeddingModelConfig) -> Optional[Any]:
        """
        åŠ è½½ Embedding æ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®
            
        Returns:
            åŠ è½½çš„æ¨¡å‹æˆ– None
        """
        try:
            print(f"   åŠ è½½æ¨¡å‹: {config.model_id}")
            print(f"   (ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼Œé¦–æ¬¡åŠ è½½éœ€è¦è€å¿ƒç­‰å¾…...)")
            
            # ä¼˜å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹
            try:
                from modelscope import snapshot_download
                print(f"   æ­£åœ¨ä» ModelScope ä¸‹è½½æ¨¡å‹: {config.model_id}...")
                
                # ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
                model_dir = snapshot_download(config.model_id, cache_dir="./model_cache")
                print(f"   âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
                
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ç±»å‹
                import os
                model_files = os.listdir(model_dir)
                print(f"   ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {[f for f in model_files if f.endswith(('.bin', '.safetensors'))]}")
                
                # ä½¿ç”¨æ‰‹åŠ¨åŠ è½½æ–¹å¼åŠ è½½ pytorch_model.bin
                if 'pytorch_model.bin' in model_files:
                    print(f"   ğŸ”„ æ£€æµ‹åˆ° pytorch_model.bin æ ¼å¼ï¼Œä½¿ç”¨æ‰‹åŠ¨åŠ è½½...")
                    model = self._load_from_pytorch_bin(model_dir)
                    if model is not None:
                        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (from ModelScope - pytorch_model.bin): {config.name}")
                        return model
                    print(f"   âš ï¸ æ‰‹åŠ¨åŠ è½½å¤±è´¥ï¼Œå°è¯• SentenceTransformer ç›´æ¥åŠ è½½...")
                
                # å¦‚æœæœ‰ safetensorsï¼Œç›´æ¥åŠ è½½
                elif any(f.endswith('.safetensors') for f in model_files):
                    print(f"   ğŸ”„ æ£€æµ‹åˆ° safetensors æ ¼å¼ï¼Œä½¿ç”¨ SentenceTransformer åŠ è½½...")
                    from sentence_transformers import SentenceTransformer
                    model = SentenceTransformer(model_dir)
                    print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (from ModelScope - safetensors): {config.name}")
                    return model
                
                # å°è¯•ç›´æ¥ä½¿ç”¨ SentenceTransformer åŠ è½½
                from sentence_transformers import SentenceTransformer
                model = SentenceTransformer(model_dir)
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (from ModelScope): {config.name}")
                return model
                
            except ImportError:
                print("   âš ï¸ modelscope åº“æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ Hugging Face...")
                # ç»§ç»­å°è¯•ä¸‹é¢çš„ Hugging Face æ–¹å¼
            except Exception as e:
                print(f"   âš ï¸ ModelScope åŠ è½½å¤±è´¥: {str(e)[:100]}")
                print("   å°è¯•ä½¿ç”¨ Hugging Face...")
            
            # å¦‚æœ ModelScope å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Hugging Face
            from sentence_transformers import SentenceTransformer
            
            # æ–¹å¼1: ç›´æ¥åŠ è½½ï¼ˆtrust_remote_codeè§£å†³æŸäº›æ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼‰
            try:
                model = SentenceTransformer(
                    config.model_id,
                    trust_remote_code=True
                )
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (from Hugging Face): {config.name}")
                return model
            except Exception as e1:
                print(f"   âš ï¸ ç›´æ¥åŠ è½½å¤±è´¥: {str(e1)[:100]}")
            
            # æ–¹å¼2: ç¦ç”¨ safetensors
            try:
                model = SentenceTransformer(
                    config.model_id,
                    trust_remote_code=True,
                    model_kwargs={"use_safetensors": False}
                )
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (ç¦ç”¨safetensors): {config.name}")
                return model
            except Exception as e2:
                print(f"   âš ï¸ ç¦ç”¨safetensorsåŠ è½½å¤±è´¥: {str(e2)[:100]}")
            
            return None
            
        except Exception as e:
            print(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {config.name} - {e}")
            return None
    
    def _load_from_pytorch_bin(self, model_dir: str) -> Optional[Any]:
        """
        æ‰‹åŠ¨åŠ è½½ pytorch_model.bin æ ¼å¼çš„æ¨¡å‹
        å…ˆè½¬æ¢ä¸º safetensors æ ¼å¼å†åŠ è½½
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ¨¡å‹æˆ– None
        """
        try:
            import torch
            import json
            from pathlib import Path
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ safetensors æ–‡ä»¶
            safetensors_path = Path(model_dir) / "model.safetensors"
            if safetensors_path.exists():
                print(f"   âœ… å‘ç°å·²è½¬æ¢çš„ safetensors æ–‡ä»¶")
                from sentence_transformers import SentenceTransformer
                model = SentenceTransformer(model_dir)
                return model
            
            # è½¬æ¢ pytorch_model.bin åˆ° safetensors
            print(f"   ğŸ”„ æ­£åœ¨è½¬æ¢ pytorch_model.bin åˆ° safetensors æ ¼å¼...")
            bin_path = Path(model_dir) / "pytorch_model.bin"
            
            # åŠ è½½é…ç½®
            with open(f"{model_dir}/config.json", 'r') as f:
                config = json.load(f)
            
            # åŠ è½½æƒé‡ï¼ˆä½¿ç”¨ weights_only=False ç»•è¿‡æ£€æŸ¥ï¼‰
            print(f"   ğŸ“¥ è¯»å– pytorch_model.bin æƒé‡...")
            state_dict = torch.load(
                str(bin_path),
                map_location='cpu',
                weights_only=False
            )
            
            # ä¿å­˜ä¸º safetensors æ ¼å¼ å°†æ¨¡å‹model.binæ–‡ä»¶è½¬ä¸ºsafetensorsæ ¼å¼
            try:
                from safetensors.torch import save_file
                save_file(state_dict, str(safetensors_path))
                print(f"   âœ… æˆåŠŸè½¬æ¢ä¸º safetensors æ ¼å¼: {safetensors_path}")
            except ImportError:
                print(f"   âš ï¸ safetensors åº“æœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
                import subprocess
                subprocess.check_call(["pip", "install", "safetensors"])
                from safetensors.torch import save_file
                save_file(state_dict, str(safetensors_path))
                print(f"   âœ… æˆåŠŸè½¬æ¢ä¸º safetensors æ ¼å¼: {safetensors_path}")
            
            # ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½æ¨¡å‹
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_dir)
            print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            return model
            
        except Exception as e:
            print(f"   âŒ æ‰‹åŠ¨åŠ è½½å¤±è´¥: {str(e)[:150]}")
            import traceback
            traceback.print_exc()
            return None
    
    def evaluate_retrieval(
        self,
        model: Any,
        model_name: str,
        documents: Dict[str, str],
        queries: List[Dict],
        k_values: List[int] = [1, 3, 5]
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡
        
        Args:
            model: Embedding æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            documents: æ–‡æ¡£åº“
            queries: æŸ¥è¯¢é›†
            k_values: Kå€¼åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"\n   ğŸ“Š è¯„ä¼°æ£€ç´¢è´¨é‡...")
        
        # æ„å»ºæ–‡æ¡£å‘é‡åº“
        doc_ids = list(documents.keys())
        doc_texts = list(documents.values())
        
        # å‘é‡åŒ–æ–‡æ¡£
        start_time = time.time()
        doc_embeddings = model.encode(doc_texts, show_progress_bar=False)
        doc_encode_time = time.time() - start_time
        
        # åˆå§‹åŒ–æŒ‡æ ‡ç´¯è®¡
        metrics_sum = {
            f"hit_rate@{k}": 0.0 for k in k_values
        }
        metrics_sum.update({
            f"precision@{k}": 0.0 for k in k_values
        })
        metrics_sum.update({
            f"recall@{k}": 0.0 for k in k_values
        })
        metrics_sum.update({
            f"ndcg@{k}": 0.0 for k in k_values
        })
        metrics_sum["mrr"] = 0.0
        
        query_results = []
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œè¯„ä¼°
        for query_info in queries:
            query = query_info["query"]
            relevant_docs = query_info["relevant_docs"]
            
            # æŸ¥è¯¢å‘é‡åŒ–
            query_embedding = model.encode([query], show_progress_bar=False)[0]
            
            # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
            similarities = []
            for idx, doc_emb in enumerate(doc_embeddings):
                sim = self.metrics.cosine_similarity(query_embedding, doc_emb)
                similarities.append((doc_ids[idx], sim))
            
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)
            retrieved_ids = [doc_id for doc_id, _ in similarities]
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            query_metrics = {}
            for k in k_values:
                query_metrics[f"hit_rate@{k}"] = self.metrics.hit_rate(retrieved_ids, relevant_docs, k)
                query_metrics[f"precision@{k}"] = self.metrics.precision_at_k(retrieved_ids, relevant_docs, k)
                query_metrics[f"recall@{k}"] = self.metrics.recall_at_k(retrieved_ids, relevant_docs, k)
                query_metrics[f"ndcg@{k}"] = self.metrics.ndcg_at_k(retrieved_ids, relevant_docs, k)
                
                metrics_sum[f"hit_rate@{k}"] += query_metrics[f"hit_rate@{k}"]
                metrics_sum[f"precision@{k}"] += query_metrics[f"precision@{k}"]
                metrics_sum[f"recall@{k}"] += query_metrics[f"recall@{k}"]
                metrics_sum[f"ndcg@{k}"] += query_metrics[f"ndcg@{k}"]
            
            mrr = self.metrics.mrr(retrieved_ids, relevant_docs)
            query_metrics["mrr"] = mrr
            metrics_sum["mrr"] += mrr
            
            query_results.append({
                "query": query,
                "relevant_docs": relevant_docs,
                "top_3_retrieved": retrieved_ids[:3],
                "top_3_scores": [s for _, s in similarities[:3]],
                "metrics": query_metrics
            })
        
        # è®¡ç®—å¹³å‡å€¼
        num_queries = len(queries)
        avg_metrics = {k: v / num_queries for k, v in metrics_sum.items()}
        
        return {
            "avg_metrics": avg_metrics,
            "query_results": query_results,
            "doc_encode_time": doc_encode_time,
            "num_documents": len(documents),
            "num_queries": num_queries
        }
    
    def evaluate_semantic_discrimination(
        self,
        model: Any,
        model_name: str,
        semantic_pairs: List[Tuple[str, str, int]]
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°è¯­ä¹‰åŒºåˆ†èƒ½åŠ›
        
        Args:
            model: Embedding æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            semantic_pairs: æ­£è´Ÿæ ·æœ¬å¯¹åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"   ğŸ“Š è¯„ä¼°è¯­ä¹‰åŒºåˆ†èƒ½åŠ›...")
        
        positive_similarities = []
        negative_similarities = []
        predictions = []
        labels = []
        
        for text1, text2, label in semantic_pairs:
            # è®¡ç®—ç›¸ä¼¼åº¦
            emb1 = model.encode([text1], show_progress_bar=False)[0]
            emb2 = model.encode([text2], show_progress_bar=False)[0]
            similarity = self.metrics.cosine_similarity(emb1, emb2)
            
            if label == 1:
                positive_similarities.append(similarity)
            else:
                negative_similarities.append(similarity)
            
            labels.append(label)
            predictions.append(similarity)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_positive_sim = np.mean(positive_similarities) if positive_similarities else 0
        avg_negative_sim = np.mean(negative_similarities) if negative_similarities else 0
        discrimination_gap = avg_positive_sim - avg_negative_sim
        
        # è®¡ç®— AUCï¼ˆä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºé¢„æµ‹åˆ†æ•°ï¼‰
        try:
            from sklearn.metrics import roc_auc_score
            auc_score = roc_auc_score(labels, predictions)
        except Exception:
            auc_score = 0.0
        
        # ä½¿ç”¨é˜ˆå€¼è®¡ç®—å‡†ç¡®ç‡ï¼ˆé˜ˆå€¼ = æ­£è´Ÿæ ·æœ¬å¹³å‡ç›¸ä¼¼åº¦çš„ä¸­ç‚¹ï¼‰
        threshold = (avg_positive_sim + avg_negative_sim) / 2
        correct = sum(
            1 for pred, label in zip(predictions, labels)
            if (pred >= threshold and label == 1) or (pred < threshold and label == 0)
        )
        accuracy = correct / len(labels) if labels else 0
        
        return {
            "avg_positive_similarity": avg_positive_sim,
            "avg_negative_similarity": avg_negative_sim,
            "discrimination_gap": discrimination_gap,
            "auc_score": auc_score,
            "accuracy": accuracy,
            "threshold": threshold,
            "positive_similarities": positive_similarities,
            "negative_similarities": negative_similarities
        }
    
    def evaluate_performance(
        self,
        model: Any,
        model_name: str,
        test_texts: List[str],
        batch_sizes: List[int] = [1, 8, 32]
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨ç†æ€§èƒ½
        
        Args:
            model: Embedding æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            test_texts: æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
            batch_sizes: æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"   ğŸ“Š è¯„ä¼°æ¨ç†æ€§èƒ½...")
        
        performance_results = {}
        
        for batch_size in batch_sizes:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            num_batches = len(test_texts) // batch_size
            if num_batches == 0:
                continue
            
            total_time = 0
            total_texts = 0
            
            for i in range(num_batches):
                batch = test_texts[i * batch_size: (i + 1) * batch_size]
                
                start_time = time.time()
                _ = model.encode(batch, show_progress_bar=False)
                elapsed = time.time() - start_time
                
                total_time += elapsed
                total_texts += len(batch)
            
            avg_latency = total_time / num_batches * 1000  # æ¯«ç§’
            throughput = total_texts / total_time if total_time > 0 else 0  # texts/sec
            
            performance_results[f"batch_{batch_size}"] = {
                "avg_latency_ms": avg_latency,
                "throughput": throughput,
                "total_texts": total_texts,
                "total_time": total_time
            }
        
        return performance_results
    
    def run_full_evaluation(self, model_configs: List[EmbeddingModelConfig] = None):
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨ EMBEDDING_MODELSï¼‰
        """
        if model_configs is None:
            model_configs = EMBEDDING_MODELS
        
        # è·å–è¯„ä¼°æ•°æ®é›†
        dataset = get_evaluation_dataset()
        documents = dataset["documents"]
        queries = dataset["queries"]
        semantic_pairs = dataset["semantic_pairs"]
        
        print("=" * 70)
        print("ğŸš€ Embedding æ¨¡å‹æ•ˆæœè¯„ä¼°")
        print("=" * 70)
        print(f"\nğŸ“„ æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(queries)}")
        print(f"   è¯­ä¹‰å¯¹æ•°é‡: {len(semantic_pairs)}")
        print(f"   å¾…è¯„ä¼°æ¨¡å‹: {len(model_configs)} ä¸ª")
        
        # æ€§èƒ½æµ‹è¯•ç”¨çš„æ–‡æœ¬
        perf_test_texts = list(documents.values()) * 3  # 45ä¸ªæ–‡æœ¬
        
        # é€ä¸ªè¯„ä¼°æ¨¡å‹
        for config in model_configs:
            print()
            print("=" * 70)
            print(f"ğŸ”¹ è¯„ä¼°æ¨¡å‹: {config.description}")
            print("=" * 70)
            
            # åŠ è½½æ¨¡å‹
            model = self.load_model(config)
            if model is None:
                print(f"   âš ï¸ è·³è¿‡æ¨¡å‹: {config.name}")
                self.results[config.name] = {"status": "failed", "error": "æ¨¡å‹åŠ è½½å¤±è´¥"}
                continue
            
            self.models[config.name] = model
            
            # 1. æ£€ç´¢è´¨é‡è¯„ä¼°
            retrieval_results = self.evaluate_retrieval(
                model, config.name, documents, queries
            )
            
            # 2. è¯­ä¹‰åŒºåˆ†èƒ½åŠ›è¯„ä¼°
            semantic_results = self.evaluate_semantic_discrimination(
                model, config.name, semantic_pairs
            )
            
            # 3. æ¨ç†æ€§èƒ½è¯„ä¼°
            performance_results = self.evaluate_performance(
                model, config.name, perf_test_texts
            )
            
            # ä¿å­˜ç»“æœ
            self.results[config.name] = {
                "status": "success",
                "config": config,
                "retrieval": retrieval_results,
                "semantic": semantic_results,
                "performance": performance_results
            }
            
            # æ‰“å°ç®€è¦ç»“æœ
            self._print_model_summary(config.name)
        
        # æ‰“å°æ€»ç»“å¯¹æ¯”
        self._print_comparison_summary()
    
    def _print_model_summary(self, model_name: str):
        """æ‰“å°å•ä¸ªæ¨¡å‹çš„è¯„ä¼°æ‘˜è¦"""
        result = self.results.get(model_name)
        if not result or result.get("status") != "success":
            return
        
        retrieval = result["retrieval"]["avg_metrics"]
        semantic = result["semantic"]
        
        print(f"\n   ğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦:")
        print(f"   {'â”€' * 40}")
        print(f"   æ£€ç´¢æŒ‡æ ‡:")
        print(f"      Hit Rate@1: {retrieval.get('hit_rate@1', 0):.1%}")
        print(f"      Hit Rate@3: {retrieval.get('hit_rate@3', 0):.1%}")
        print(f"      MRR: {retrieval.get('mrr', 0):.3f}")
        print(f"      NDCG@3: {retrieval.get('ndcg@3', 0):.3f}")
        print(f"   è¯­ä¹‰åŒºåˆ†:")
        print(f"      æ­£æ ·æœ¬å¹³å‡ç›¸ä¼¼åº¦: {semantic['avg_positive_similarity']:.3f}")
        print(f"      è´Ÿæ ·æœ¬å¹³å‡ç›¸ä¼¼åº¦: {semantic['avg_negative_similarity']:.3f}")
        print(f"      åŒºåˆ†åº¦: {semantic['discrimination_gap']:.3f}")
        print(f"      AUC: {semantic['auc_score']:.3f}")
    
    def _print_comparison_summary(self):
        """æ‰“å°æ¨¡å‹å¯¹æ¯”æ€»ç»“"""
        print()
        print("=" * 70)
        print("ğŸ“Š æ¨¡å‹å¯¹æ¯”æ€»ç»“")
        print("=" * 70)
        
        # æ”¶é›†æˆåŠŸçš„æ¨¡å‹ç»“æœ
        valid_results = [
            (name, result) for name, result in self.results.items()
            if result.get("status") == "success"
        ]
        
        if not valid_results:
            print("\nâš ï¸ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
            return
        
        # 1. æ£€ç´¢è´¨é‡å¯¹æ¯”
        print("\n1ï¸âƒ£ æ£€ç´¢è´¨é‡å¯¹æ¯”")
        print("-" * 70)
        print(f"{'æ¨¡å‹':<25} {'Hit@1':<10} {'Hit@3':<10} {'MRR':<10} {'NDCG@3':<10}")
        print("-" * 70)
        
        retrieval_scores = []
        for name, result in valid_results:
            metrics = result["retrieval"]["avg_metrics"]
            hit1 = metrics.get("hit_rate@1", 0)
            hit3 = metrics.get("hit_rate@3", 0)
            mrr = metrics.get("mrr", 0)
            ndcg3 = metrics.get("ndcg@3", 0)
            
            # ç»¼åˆå¾—åˆ†
            composite = hit1 * 0.3 + hit3 * 0.2 + mrr * 0.3 + ndcg3 * 0.2
            retrieval_scores.append((name, composite))
            
            print(f"{name:<25} {hit1:<10.1%} {hit3:<10.1%} {mrr:<10.3f} {ndcg3:<10.3f}")
        
        # 2. è¯­ä¹‰åŒºåˆ†èƒ½åŠ›å¯¹æ¯”
        print("\n2ï¸âƒ£ è¯­ä¹‰åŒºåˆ†èƒ½åŠ›å¯¹æ¯”")
        print("-" * 70)
        print(f"{'æ¨¡å‹':<25} {'æ­£æ ·æœ¬ç›¸ä¼¼åº¦':<15} {'è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦':<15} {'åŒºåˆ†åº¦':<10} {'AUC':<10}")
        print("-" * 70)
        
        semantic_scores = []
        for name, result in valid_results:
            sem = result["semantic"]
            pos = sem["avg_positive_similarity"]
            neg = sem["avg_negative_similarity"]
            gap = sem["discrimination_gap"]
            auc = sem["auc_score"]
            
            semantic_scores.append((name, auc))
            print(f"{name:<25} {pos:<15.3f} {neg:<15.3f} {gap:<10.3f} {auc:<10.3f}")
        
        # 3. æ¨ç†æ€§èƒ½å¯¹æ¯”
        print("\n3ï¸âƒ£ æ¨ç†æ€§èƒ½å¯¹æ¯” (batch_size=8)")
        print("-" * 70)
        print(f"{'æ¨¡å‹':<25} {'å»¶è¿Ÿ(ms)':<15} {'ååé‡(texts/s)':<20}")
        print("-" * 70)
        
        performance_scores = []
        for name, result in valid_results:
            perf = result["performance"]
            if "batch_8" in perf:
                latency = perf["batch_8"]["avg_latency_ms"]
                throughput = perf["batch_8"]["throughput"]
                performance_scores.append((name, throughput))
                print(f"{name:<25} {latency:<15.1f} {throughput:<20.1f}")
            else:
                print(f"{name:<25} {'N/A':<15} {'N/A':<20}")
        
        # 4. ç»¼åˆè¯„åˆ†
        print("\n4ï¸âƒ£ ç»¼åˆè¯„åˆ†æ’å")
        print("-" * 70)
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆæ£€ç´¢50% + è¯­ä¹‰30% + æ€§èƒ½20%ï¼‰
        composite_scores = []
        for name, result in valid_results:
            retrieval = result["retrieval"]["avg_metrics"]
            semantic = result["semantic"]
            
            # æ£€ç´¢å¾—åˆ† (å½’ä¸€åŒ–åˆ°0-1)
            ret_score = (
                retrieval.get("hit_rate@1", 0) * 0.3 +
                retrieval.get("hit_rate@3", 0) * 0.2 +
                retrieval.get("mrr", 0) * 0.3 +
                retrieval.get("ndcg@3", 0) * 0.2
            )
            
            # è¯­ä¹‰å¾—åˆ†
            sem_score = semantic["auc_score"]
            
            # æ€§èƒ½å¾—åˆ†ï¼ˆç›¸å¯¹åˆ†æ•°ï¼‰
            perf = result["performance"]
            perf_score = 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            if "batch_8" in perf and performance_scores:
                max_throughput = max(s[1] for s in performance_scores if s[1] > 0)
                if max_throughput > 0:
                    perf_score = perf["batch_8"]["throughput"] / max_throughput
            
            # ç»¼åˆå¾—åˆ†
            composite = ret_score * 0.5 + sem_score * 0.3 + perf_score * 0.2
            composite_scores.append((name, composite, ret_score, sem_score, perf_score))
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        composite_scores.sort(key=lambda x: x[1], reverse=True)
        
        print(f"{'æ’å':<6} {'æ¨¡å‹':<25} {'ç»¼åˆå¾—åˆ†':<12} {'æ£€ç´¢':<10} {'è¯­ä¹‰':<10} {'æ€§èƒ½':<10}")
        print("-" * 70)
        
        for rank, (name, composite, ret, sem, perf) in enumerate(composite_scores, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(f"{medal}{rank:<5} {name:<25} {composite:<12.3f} {ret:<10.3f} {sem:<10.3f} {perf:<10.3f}")
        
        # æ¨è
        if composite_scores:
            best = composite_scores[0]
            print()
            print("=" * 70)
            print("ğŸ’¡ è¯„ä¼°ç»“è®º")
            print("=" * 70)
            print(f"\nğŸ† ç»¼åˆæœ€ä½³æ¨¡å‹: {best[0]}")
            print(f"   ç»¼åˆå¾—åˆ†: {best[1]:.3f}")
            print()
            
            # å„ç»´åº¦æœ€ä½³
            best_retrieval = max(valid_results, key=lambda x: x[1]["retrieval"]["avg_metrics"].get("mrr", 0))
            best_semantic = max(valid_results, key=lambda x: x[1]["semantic"]["auc_score"])
            
            print("ğŸ“‹ å„ç»´åº¦æœ€ä½³:")
            print(f"   â€¢ æ£€ç´¢è´¨é‡æœ€ä½³: {best_retrieval[0]}")
            print(f"   â€¢ è¯­ä¹‰åŒºåˆ†æœ€ä½³: {best_semantic[0]}")
            if performance_scores:
                best_perf = max(performance_scores, key=lambda x: x[1])
                print(f"   â€¢ æ¨ç†é€Ÿåº¦æœ€å¿«: {best_perf[0]}")

    def generate_visualization(self, output_dir: str = "./eval_results"):
        """
        ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
            matplotlib.rcParams['axes.unicode_minus'] = False
        except ImportError:
            print("\nâš ï¸ æœªå®‰è£… matplotlibï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        valid_results = [
            (name, result) for name, result in self.results.items()
            if result.get("status") == "success"
        ]
        
        if not valid_results:
            return
        
        print()
        print("=" * 70)
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("=" * 70)
        
        # 1. æ£€ç´¢æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        model_names = [name for name, _ in valid_results]
        
        # æ£€ç´¢æŒ‡æ ‡
        hit1_scores = [r["retrieval"]["avg_metrics"].get("hit_rate@1", 0) for _, r in valid_results]
        hit3_scores = [r["retrieval"]["avg_metrics"].get("hit_rate@3", 0) for _, r in valid_results]
        mrr_scores = [r["retrieval"]["avg_metrics"].get("mrr", 0) for _, r in valid_results]
        
        x = np.arange(len(model_names))
        width = 0.25
        
        axes[0].bar(x - width, hit1_scores, width, label='Hit@1', color='#2ecc71')
        axes[0].bar(x, hit3_scores, width, label='Hit@3', color='#3498db')
        axes[0].bar(x + width, mrr_scores, width, label='MRR', color='#e74c3c')
        axes[0].set_xlabel('Model')
        axes[0].set_ylabel('Score')
        axes[0].set_title('Retrieval Quality Comparison')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(model_names, rotation=45, ha='right')
        axes[0].legend()
        axes[0].set_ylim(0, 1.1)
        
        # è¯­ä¹‰åŒºåˆ†èƒ½åŠ›
        pos_sims = [r["semantic"]["avg_positive_similarity"] for _, r in valid_results]
        neg_sims = [r["semantic"]["avg_negative_similarity"] for _, r in valid_results]
        
        axes[1].bar(x - width/2, pos_sims, width, label='Positive Similarity', color='#27ae60')
        axes[1].bar(x + width/2, neg_sims, width, label='Negative Similarity', color='#c0392b')
        axes[1].set_xlabel('Model')
        axes[1].set_ylabel('Similarity')
        axes[1].set_title('Semantic Discrimination Comparison')
        axes[1].set_xticks(x)
        axes[1].set_xticklabels(model_names, rotation=45, ha='right')
        axes[1].legend()
        
        plt.tight_layout()
        chart_path = os.path.join(output_dir, "embedding_comparison.png")
        plt.savefig(chart_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ä¿å­˜å›¾è¡¨: {chart_path}")
        
        # 2. AUC æŒ‡æ ‡å¯¹æ¯”å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        
        auc_scores = [r["semantic"]["auc_score"] for _, r in valid_results]
        discrimination_gaps = [r["semantic"]["discrimination_gap"] for _, r in valid_results]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, auc_scores, width, label='AUC Score', color='#9b59b6', alpha=0.8)
        bars2 = ax.bar(x + width/2, discrimination_gaps, width, label='Discrimination Gap', color='#f39c12', alpha=0.8)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar in bars1:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=9)
        
        for bar in bars2:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=9)
        
        ax.set_xlabel('Model')
        ax.set_ylabel('Score')
        ax.set_title('AUC Score and Discrimination Gap Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim(0, 1.1)
        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Random Guess (0.5)')
        ax.legend()
        
        plt.tight_layout()
        auc_chart_path = os.path.join(output_dir, "auc_comparison.png")
        plt.savefig(auc_chart_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ä¿å­˜å›¾è¡¨: {auc_chart_path}")
        
        # 3. ç›¸ä¼¼åº¦åˆ†å¸ƒç®±çº¿å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        
        all_data = []
        all_labels = []
        for name, result in valid_results:
            pos = result["semantic"]["positive_similarities"]
            neg = result["semantic"]["negative_similarities"]
            all_data.extend([pos, neg])
            all_labels.extend([f"{name}\n(Positive)", f"{name}\n(Negative)"])
        
        bp = ax.boxplot(all_data, labels=all_labels, patch_artist=True)
        
        colors = ['#2ecc71', '#e74c3c'] * len(valid_results)
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.6)
        
        ax.set_ylabel('Cosine Similarity')
        ax.set_title('Similarity Distribution by Model')
        plt.xticks(rotation=45, ha='right')
        
        plt.tight_layout()
        boxplot_path = os.path.join(output_dir, "similarity_distribution.png")
        plt.savefig(boxplot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ä¿å­˜å›¾è¡¨: {boxplot_path}")
        
        # 4. ç»¼åˆæŒ‡æ ‡é›·è¾¾å›¾
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        categories = ['Hit@1', 'Hit@3', 'MRR', 'AUC', 'Discrimination Gap']
        N = len(categories)
        
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶é›·è¾¾å›¾
        colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']
        for idx, (name, result) in enumerate(valid_results):
            values = [
                result["retrieval"]["avg_metrics"].get("hit_rate@1", 0),
                result["retrieval"]["avg_metrics"].get("hit_rate@3", 0),
                result["retrieval"]["avg_metrics"].get("mrr", 0),
                result["semantic"]["auc_score"],
                result["semantic"]["discrimination_gap"]
            ]
            values += values[:1]  # é—­åˆå›¾å½¢
            
            color = colors[idx % len(colors)]
            ax.plot(angles, values, 'o-', linewidth=2, label=name, color=color)
            ax.fill(angles, values, alpha=0.15, color=color)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1.1)
        ax.set_title('Comprehensive Performance Radar Chart', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)
        
        plt.tight_layout()
        radar_path = os.path.join(output_dir, "radar_comparison.png")
        plt.savefig(radar_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ä¿å­˜å›¾è¡¨: {radar_path}")
        
        print(f"\nğŸ“ å›¾è¡¨ä¿å­˜ç›®å½•: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " Embedding æ¨¡å‹æ•ˆæœè¯„ä¼°å·¥å…· ".center(58) + "        â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        from sentence_transformers import SentenceTransformer
        import numpy as np
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("\nè¯·å®‰è£…ä¾èµ–ï¼š")
        print("pip install sentence-transformers numpy matplotlib scikit-learn")
        sys.exit(1)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EmbeddingEvaluator()
    
    try:
        # è¿è¡Œè¯„ä¼°
        evaluator.run_full_evaluation()
        
        # ç”Ÿæˆå¯è§†åŒ–
        evaluator.generate_visualization()
        
        print()
        print("=" * 70)
        print("âœ… è¯„ä¼°å®Œæˆï¼")
        print("=" * 70)
        print()
        print("ğŸ“ æŒ‡æ ‡è¯´æ˜ï¼š")
        print("   â€¢ Hit Rate@K: Top-Kç»“æœä¸­å‘½ä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹")
        print("   â€¢ MRR: ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åçš„å€’æ•°å‡å€¼")
        print("   â€¢ NDCG@K: è€ƒè™‘æ’åºä½ç½®çš„å½’ä¸€åŒ–æŠ˜æŸç´¯è®¡å¢ç›Š")
        print("   â€¢ åŒºåˆ†åº¦: æ­£æ ·æœ¬ä¸è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦çš„å·®å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰")
        print("   â€¢ AUC: ROCæ›²çº¿ä¸‹é¢ç§¯ï¼Œåæ˜ åˆ†ç±»èƒ½åŠ›ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰")
        print()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print("ğŸ‘‹ ç¨‹åºç»“æŸ")
    sys.exit(0)


if __name__ == "__main__":
    main()

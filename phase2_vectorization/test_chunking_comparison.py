#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å—ç­–ç•¥æ•ˆæœå¯¹æ¯”æµ‹è¯•

æœ¬è„šæœ¬æµ‹è¯•ä¸åŒåˆ†å—é…ç½®ä¸‹çš„ RAG æ£€ç´¢æ•ˆæœï¼š
1. ä½¿ç”¨ BGE-large-zh æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
2. å°†ä¸åŒé…ç½®çš„åˆ†å—å†™å…¥ ChromaDB
3. é€šè¿‡ç›¸ä¼¼åº¦æœç´¢å¯¹æ¯”ä¸åŒé…ç½®çš„æ£€ç´¢æ•ˆæœ

ä¾èµ–å®‰è£…ï¼š
pip install langchain-text-splitters chromadb sentence-transformers
"""

import os
import shutil
import time
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass


# ============================================================
# åˆ†å—é…ç½®å®šä¹‰
# ============================================================
@dataclass
class ChunkConfig:
    """åˆ†å—é…ç½®ç±»"""
    name: str
    chunk_size: int
    chunk_overlap: int
    description: str


# å®šä¹‰ä¸åŒçš„åˆ†å—é…ç½®
CHUNK_CONFIGS = [
    ChunkConfig(
        name="config_small",
        chunk_size=120,
        chunk_overlap=0,
        description="å°å—é…ç½®(120/0) - å¥å­çº§åˆ«ï¼Œæ— é‡å "
    ),
    ChunkConfig(
        name="config_qa",
        chunk_size=150,
        chunk_overlap=20,
        description="é—®ç­”é…ç½®(150/20) - é«˜ç²¾åº¦é—®ç­”åœºæ™¯"
    ),
    ChunkConfig(
        name="config_standard",
        chunk_size=180,
        chunk_overlap=30,
        description="æ ‡å‡†é…ç½®(180/30) - BGEæ¨èé…ç½®"
    ),
    ChunkConfig(
        name="config_large",
        chunk_size=250,
        chunk_overlap=50,
        description="å¤§å—é…ç½®(250/50) - æ›´å¤šä¸Šä¸‹æ–‡"
    ),
]

# ä¸­æ–‡åˆ†éš”ç¬¦ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
CHINESE_SEPARATORS = [
    "\n\n",    # æ®µè½
    "\n",      # æ¢è¡Œ
    "ã€‚",      # å¥å·
    "ï¼",      # æ„Ÿå¹å·
    "ï¼Ÿ",      # é—®å·
    "ï¼›",      # åˆ†å·
    "ï¼š",      # å†’å·
    "ï¼Œ",      # é€—å·
    "ã€",      # é¡¿å·
    " ",       # ç©ºæ ¼
    ""         # å­—ç¬¦ï¼ˆå…œåº•ï¼‰
]


def get_sample_documents() -> str:
    """è·å–æµ‹è¯•ç”¨çš„ä¸­æ–‡æ–‡æ¡£"""
    return """RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚å®ƒé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›æ›´å‡†ç¡®çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œç”Ÿæˆæ›´å¯é çš„å›ç­”ã€‚RAGæŠ€æœ¯æœ‰æ•ˆå‡å°‘äº†å¤§æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚

RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼šæ–‡æ¡£å¤„ç†æ¨¡å—ã€å‘é‡åŒ–æ¨¡å—ã€å‘é‡æ•°æ®åº“å’Œç”Ÿæˆæ¨¡å—ã€‚æ–‡æ¡£å¤„ç†è´Ÿè´£å°†åŸå§‹æ–‡æ¡£è¿›è¡Œæ¸…æ´—å’Œåˆ†å—ï¼›å‘é‡åŒ–å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼›å‘é‡æ•°æ®åº“å­˜å‚¨å’Œæ£€ç´¢å‘é‡ï¼›ç”Ÿæˆæ¨¡å—åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆã€‚

å‘é‡æ•°æ®åº“æ˜¯RAGç³»ç»Ÿä¸­å­˜å‚¨å’Œæ£€ç´¢æ–‡æœ¬å‘é‡çš„æ ¸å¿ƒç»„ä»¶ã€‚å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬Chromaã€FAISSã€Milvusã€Pineconeã€Weaviateç­‰ã€‚Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºå‘é‡æ•°æ®åº“ï¼Œé€‚åˆå¿«é€ŸåŸå‹å¼€å‘ã€‚FAISSæ˜¯Facebookå¼€å‘çš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“ï¼Œæ”¯æŒæµ·é‡æ•°æ®ã€‚

Embeddingæ¨¡å‹è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚ä¸­æ–‡åœºæ™¯å¸¸ç”¨çš„Embeddingæ¨¡å‹åŒ…æ‹¬BGEç³»åˆ—ã€M3Eç³»åˆ—ã€text2vec-chineseç­‰ã€‚BGE-large-zhæ˜¯åŒ—äº¬æ™ºæºç ”ç©¶é™¢å¼€å‘çš„ä¸­æ–‡Embeddingæ¨¡å‹ï¼Œåœ¨å¤šä¸ªä¸­æ–‡è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæœ€å¤§æ”¯æŒ512ä¸ªtokenè¾“å…¥ã€‚

æ–‡æ¡£åˆ†å—æ˜¯RAGç³»ç»Ÿä¸­éå¸¸å…³é”®çš„ç¯èŠ‚ã€‚åˆ†å—ç­–ç•¥ç›´æ¥å½±å“æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è´¨é‡ã€‚å¸¸è§çš„åˆ†å—ç­–ç•¥åŒ…æ‹¬ï¼šå›ºå®šå¤§å°åˆ†å—ã€åŸºäºå¥å­åˆ†å—ã€åŸºäºæ®µè½åˆ†å—ã€è¯­ä¹‰åˆ†å—ç­‰ã€‚åˆ†å—å¤§å°éœ€è¦æ ¹æ®Embeddingæ¨¡å‹çš„tokené™åˆ¶æ¥è®¾ç½®ã€‚

LangChainæ˜¯ä¸€ä¸ªæµè¡Œçš„å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæä¾›äº†ä¸°å¯Œçš„å·¥å…·æ¥æ„å»ºRAGç³»ç»Ÿã€‚å®ƒæ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨ã€å‘é‡æ•°æ®åº“å’ŒLLMé›†æˆã€‚ä½¿ç”¨LangChainå¯ä»¥å¿«é€Ÿæ­å»ºRAGåº”ç”¨åŸå‹ã€‚

æ£€ç´¢ç­–ç•¥å¯¹RAGæ•ˆæœæœ‰é‡è¦å½±å“ã€‚å¸¸è§çš„æ£€ç´¢ç­–ç•¥åŒ…æ‹¬ï¼šç›¸ä¼¼åº¦æ£€ç´¢ã€æ··åˆæ£€ç´¢ï¼ˆç»“åˆå…³é”®è¯å’Œè¯­ä¹‰ï¼‰ã€é‡æ’åºã€å¤šè·¯å¬å›ç­‰ã€‚é€‰æ‹©åˆé€‚çš„æ£€ç´¢ç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡å›ç­”è´¨é‡ã€‚

RAGçš„è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬æ£€ç´¢å‡†ç¡®ç‡ã€å›ç­”ç›¸å…³æ€§ã€å›ç­”å‡†ç¡®æ€§ã€å“åº”å»¶è¿Ÿç­‰ã€‚å¯ä»¥ä½¿ç”¨RAGASç­‰æ¡†æ¶è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚è¯„ä¼°ç»“æœæœ‰åŠ©äºä¼˜åŒ–RAGç³»ç»Ÿçš„å„ä¸ªç»„ä»¶ã€‚

å¤§æ¨¡å‹å¹»è§‰æ˜¯æŒ‡æ¨¡å‹ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…é”™è¯¯çš„å†…å®¹ã€‚RAGé€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†åº“ï¼Œè®©æ¨¡å‹çš„å›ç­”æœ‰æ®å¯ä¾ï¼Œæœ‰æ•ˆå‡å°‘äº†å¹»è§‰é—®é¢˜ã€‚è¿™æ˜¯RAGæŠ€æœ¯çš„æ ¸å¿ƒä»·å€¼ä¹‹ä¸€ã€‚

çŸ¥è¯†åº“çš„è´¨é‡ç›´æ¥å½±å“RAGç³»ç»Ÿçš„æ•ˆæœã€‚é«˜è´¨é‡çš„çŸ¥è¯†åº“åº”è¯¥å…·å¤‡ï¼šå†…å®¹å‡†ç¡®ã€æ›´æ–°åŠæ—¶ã€è¦†ç›–å…¨é¢ã€ç»“æ„æ¸…æ™°ç­‰ç‰¹ç‚¹ã€‚å®šæœŸç»´æŠ¤å’Œæ›´æ–°çŸ¥è¯†åº“æ˜¯ä¿æŒRAGç³»ç»Ÿæ•ˆæœçš„å…³é”®ã€‚"""


def get_test_queries() -> List[Dict[str, str]]:
    """è·å–æµ‹è¯•æŸ¥è¯¢å’ŒæœŸæœ›ç­”æ¡ˆ"""
    return [
        {
            "query": "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
            "expected_keywords": ["æ£€ç´¢å¢å¼ºç”Ÿæˆ", "ä¿¡æ¯æ£€ç´¢", "æ–‡æœ¬ç”Ÿæˆ", "çŸ¥è¯†åº“"],
            "category": "åŸºç¡€æ¦‚å¿µ"
        },
        {
            "query": "å¸¸è§çš„å‘é‡æ•°æ®åº“æœ‰å“ªäº›ï¼Ÿ",
            "expected_keywords": ["Chroma", "FAISS", "Milvus", "Pinecone"],
            "category": "ç»„ä»¶ä»‹ç»"
        },
        {
            "query": "BGEæ¨¡å‹çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_keywords": ["BGE", "ä¸­æ–‡", "Embedding", "512", "token"],
            "category": "æ¨¡å‹ç›¸å…³"
        },
        {
            "query": "å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿçš„æ•ˆæœï¼Ÿ",
            "expected_keywords": ["è¯„ä¼°", "å‡†ç¡®ç‡", "RAGAS", "ç›¸å…³æ€§"],
            "category": "è¯„ä¼°æ–¹æ³•"
        },
        {
            "query": "åˆ†å—ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ",
            "expected_keywords": ["å›ºå®šå¤§å°", "å¥å­", "æ®µè½", "è¯­ä¹‰"],
            "category": "æŠ€æœ¯ç»†èŠ‚"
        },
        {
            "query": "RAGå¦‚ä½•è§£å†³å¤§æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Ÿ",
            "expected_keywords": ["å¹»è§‰", "çŸ¥è¯†åº“", "æœ‰æ®å¯ä¾", "å‡å°‘"],
            "category": "æ ¸å¿ƒä»·å€¼"
        },
    ]


def split_text_with_config(text: str, config: ChunkConfig) -> List[str]:
    """ä½¿ç”¨æŒ‡å®šé…ç½®åˆ†å‰²æ–‡æœ¬"""
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        separators=CHINESE_SEPARATORS,
        length_function=len,
        keep_separator=True
    )

    return splitter.split_text(text)


class ChunkingComparison:
    """åˆ†å—ç­–ç•¥å¯¹æ¯”æµ‹è¯•ç±»"""

    def __init__(self, persist_directory: str = "./chroma_comparison_db"):
        """
        åˆå§‹åŒ–æµ‹è¯•ç±»

        Args:
            persist_directory: ChromaDB æŒä¹…åŒ–ç›®å½•
        """
        self.persist_directory = persist_directory
        self.embedding_model = None
        self.chroma_client = None
        self.collections: Dict[str, Any] = {}

    def setup(self):
        """åˆå§‹åŒ– Embedding æ¨¡å‹å’Œ ChromaDB"""
        print("=" * 70)
        print("ğŸš€ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ")
        print("=" * 70)

        # æ¸…ç†æ—§æ•°æ®
        if os.path.exists(self.persist_directory):
            print(f"   æ¸…ç†æ—§æ•°æ®ç›®å½•: {self.persist_directory}")
            shutil.rmtree(self.persist_directory)

        # åˆå§‹åŒ– Embedding æ¨¡å‹
        print("   åŠ è½½ Embedding æ¨¡å‹: BAAI/bge-large-zh-v1.5")
        print("   (ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼Œé¦–æ¬¡åŠ è½½éœ€è¦è€å¿ƒç­‰å¾…...)")

        try:
            # æ–¹å¼1: ä» ModelScope ä¸‹è½½æ¨¡å‹
            try:
                from modelscope import snapshot_download
                print("   æ­£åœ¨ä» ModelScope ä¸‹è½½æ¨¡å‹...")
                
                # ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
                model_dir = snapshot_download('BAAI/bge-large-zh-v1.5', local_dir="./models")
                print(f"   âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
                
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ç±»å‹
                import os
                model_files = os.listdir(model_dir)
                print(f"   ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {[f for f in model_files if f.endswith(('.bin', '.safetensors'))]}")
                
                # ä½¿ç”¨æ‰‹åŠ¨åŠ è½½æ–¹å¼åŠ è½½ pytorch_model.bin
                if 'pytorch_model.bin' in model_files:
                    print(f"   ğŸ”„ æ£€æµ‹åˆ° pytorch_model.bin æ ¼å¼ï¼Œä½¿ç”¨æ‰‹åŠ¨åŠ è½½...")
                    self.embedding_model = self._load_from_pytorch_bin(model_dir)
                    print("   âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ (from ModelScope - pytorch_model.bin)")
                else:
                    # ç›´æ¥ä½¿ç”¨ SentenceTransformer åŠ è½½
                    from sentence_transformers import SentenceTransformer
                    self.embedding_model = SentenceTransformer(model_dir)
                    print("   âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ (from ModelScope)")
                
            except ImportError:
                print("   âš ï¸ modelscope åº“æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ Hugging Face...")
                raise Exception("è¯·å…ˆå®‰è£…: pip install modelscope")
            except Exception as e:
                print(f"   âš ï¸ ModelScope åŠ è½½å¤±è´¥: {e}")
                raise
                
        except Exception as e:
            print(f"   âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   å°è¯•ä½¿ç”¨å¤‡é€‰æ¨¡å‹: BAAI/bge-base-zh-v1.5")
            
            try:
                from modelscope import snapshot_download
                model_dir = snapshot_download('BAAI/bge-base-zh-v1.5', local_dir="./models")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰‹åŠ¨åŠ è½½
                import os
                if 'pytorch_model.bin' in os.listdir(model_dir):
                    self.embedding_model = self._load_from_pytorch_bin(model_dir)
                else:
                    from sentence_transformers import SentenceTransformer
                    self.embedding_model = SentenceTransformer(model_dir)
                    
                print("   âœ… å¤‡é€‰æ¨¡å‹åŠ è½½å®Œæˆ (from ModelScope)")
            except Exception as e2:
                print(f"   âš ï¸ å¤‡é€‰æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                # æœ€ç»ˆå›é€€åˆ° Hugging Face
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
                print("   âœ… ä½¿ç”¨ Hugging Face åŠ è½½å¤‡é€‰æ¨¡å‹")
    
    def _load_from_pytorch_bin(self, model_dir: str):
        """
        æ‰‹åŠ¨åŠ è½½ pytorch_model.bin æ ¼å¼çš„æ¨¡å‹
        å…ˆè½¬æ¢ä¸º safetensors æ ¼å¼å†åŠ è½½
        """
        import torch
        import json
        from pathlib import Path
        
        print(f"   ğŸ”„ æ­£åœ¨è½¬æ¢ pytorch_model.bin åˆ° safetensors æ ¼å¼...")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ safetensors æ–‡ä»¶
        safetensors_path = Path(model_dir) / "model.safetensors"
        if safetensors_path.exists():
            print(f"   âœ… å‘ç°å·²è½¬æ¢çš„ safetensors æ–‡ä»¶")
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_dir)
            return model
        
        # è½¬æ¢ pytorch_model.bin åˆ° safetensors
        bin_path = Path(model_dir) / "pytorch_model.bin"
        
        # åŠ è½½é…ç½®
        with open(f"{model_dir}/config.json", 'r') as f:
            config = json.load(f)
        
        # åŠ è½½æƒé‡ï¼ˆä½¿ç”¨ weights_only=False ç»•è¿‡æ£€æŸ¥ï¼‰
        print(f"   ğŸ“¥ è¯»å– pytorch_model.bin æƒé‡...")
        state_dict = torch.load(
            str(bin_path),
            map_location='cpu',
            weights_only=False
        )
        
        # ä¿å­˜ä¸º safetensors æ ¼å¼
        try:
            from safetensors.torch import save_file
            save_file(state_dict, str(safetensors_path))
            print(f"   âœ… æˆåŠŸè½¬æ¢ä¸º safetensors æ ¼å¼: {safetensors_path}")
        except ImportError:
            print(f"   âš ï¸ safetensors åº“æœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
            import subprocess
            subprocess.check_call(["pip", "install", "safetensors"])
            from safetensors.torch import save_file
            save_file(state_dict, str(safetensors_path))
            print(f"   âœ… æˆåŠŸè½¬æ¢ä¸º safetensors æ ¼å¼: {safetensors_path}")
        
        # ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½æ¨¡å‹
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(model_dir)
        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return model

        # åˆå§‹åŒ– ChromaDB
        print("   åˆå§‹åŒ– ChromaDB...")
        import chromadb
        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)
        print("   âœ… ChromaDB åˆå§‹åŒ–å®Œæˆ")
        print()

    def create_collections(self, configs: List[ChunkConfig], text: str):
        """ä¸ºæ¯ä¸ªé…ç½®åˆ›å»º Collection å¹¶å†™å…¥æ•°æ®"""
        print("=" * 70)
        print("ğŸ“¦ åˆ›å»º Collections å¹¶å†™å…¥æ•°æ®")
        print("=" * 70)

        for config in configs:
            print(f"\nğŸ”¹ {config.description}")
            print("-" * 50)

            # åˆ†å—
            chunks = split_text_with_config(text, config)
            print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
            print(f"   å¹³å‡é•¿åº¦: {sum(len(c) for c in chunks) / len(chunks):.1f} å­—ç¬¦")

            # åˆ›å»º Collection
            collection = self.chroma_client.create_collection(
                name=config.name,
                metadata={"description": config.description}
            )

            # å‘é‡åŒ–
            print("   å‘é‡åŒ–ä¸­...")
            start_time = time.time()
            embeddings = self.embedding_model.encode(chunks, show_progress_bar=False)
            embed_time = time.time() - start_time
            print(f"   å‘é‡åŒ–è€—æ—¶: {embed_time:.2f}ç§’")

            # å†™å…¥ ChromaDB
            ids = [f"{config.name}_chunk_{i}" for i in range(len(chunks))]
            metadatas = [{"chunk_index": i, "chunk_size": len(chunk)} for i, chunk in enumerate(chunks)]

            collection.add(
                ids=ids,
                embeddings=embeddings.tolist(),
                documents=chunks,
                metadatas=metadatas
            )
            print(f"   âœ… å†™å…¥ {len(chunks)} æ¡è®°å½•åˆ° Collection: {config.name}")

            self.collections[config.name] = {
                "collection": collection,
                "config": config,
                "chunks": chunks,
                "chunk_count": len(chunks)
            }

        print()

    def search(self, query: str, n_results: int = 3) -> Dict[str, List[Dict]]:
        """åœ¨æ‰€æœ‰ Collection ä¸­æœç´¢"""
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.embedding_model.encode([query])[0].tolist()

        results = {}
        for name, data in self.collections.items():
            collection = data["collection"]

            search_result = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=["documents", "distances", "metadatas"]
            )

            results[name] = {
                "documents": search_result["documents"][0],
                "distances": search_result["distances"][0],
                "metadatas": search_result["metadatas"][0],
                "config": data["config"]
            }

        return results

    def evaluate_results(self, results: Dict, expected_keywords: List[str]) -> Dict[str, Dict]:
        """è¯„ä¼°æœç´¢ç»“æœ"""
        evaluation = {}

        for config_name, result in results.items():
            documents = result["documents"]
            distances = result["distances"]

            # è®¡ç®—å…³é”®è¯å‘½ä¸­ç‡
            all_text = " ".join(documents)
            hits = sum(1 for kw in expected_keywords if kw in all_text)
            keyword_coverage = hits / len(expected_keywords) * 100

            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            avg_distance = sum(distances) / len(distances)
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰
            avg_similarity = (1 - avg_distance) * 100 if avg_distance < 1 else 0

            # è®¡ç®—ç»“æœå¤šæ ·æ€§ï¼ˆä¸åŒchunkçš„æ•°é‡ï¼‰
            unique_chunks = len(set(documents))

            evaluation[config_name] = {
                "keyword_coverage": keyword_coverage,
                "avg_similarity": avg_similarity,
                "avg_distance": avg_distance,
                "unique_chunks": unique_chunks,
                "top_distance": distances[0] if distances else 1.0,
                "description": result["config"].description
            }

        return evaluation

    def run_comparison_test(self, queries: List[Dict]) -> Dict:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("=" * 70)
        print("ğŸ” æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢å¯¹æ¯”æµ‹è¯•")
        print("=" * 70)

        all_evaluations = {config.name: {
            "total_keyword_coverage": 0,
            "total_similarity": 0,
            "query_count": 0,
            "best_match_count": 0,
            "description": ""
        } for config in CHUNK_CONFIGS}

        for i, query_info in enumerate(queries, 1):
            query = query_info["query"]
            expected_keywords = query_info["expected_keywords"]
            category = query_info["category"]

            print(f"\nğŸ“Œ æµ‹è¯• {i}/{len(queries)}: {query}")
            print(f"   ç±»åˆ«: {category}")
            print(f"   æœŸæœ›å…³é”®è¯: {', '.join(expected_keywords)}")
            print("-" * 50)

            # æœç´¢
            results = self.search(query, n_results=3)

            # è¯„ä¼°
            evaluation = self.evaluate_results(results, expected_keywords)

            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best_config = max(evaluation.items(),
                              key=lambda x: (x[1]["keyword_coverage"], x[1]["avg_similarity"]))

            # æ‰“å°ç»“æœ
            print(f"\n{'é…ç½®':<25} {'å…³é”®è¯è¦†ç›–':<12} {'ç›¸ä¼¼åº¦':<12} {'Topè·ç¦»':<12}")
            print("-" * 60)

            for config_name, eval_result in evaluation.items():
                is_best = "â­" if config_name == best_config[0] else "  "
                print(f"{is_best}{eval_result['description'][:22]:<23} "
                      f"{eval_result['keyword_coverage']:.1f}%{'':<7} "
                      f"{eval_result['avg_similarity']:.1f}%{'':<7} "
                      f"{eval_result['top_distance']:.4f}")

                # ç´¯è®¡ç»Ÿè®¡
                all_evaluations[config_name]["total_keyword_coverage"] += eval_result["keyword_coverage"]
                all_evaluations[config_name]["total_similarity"] += eval_result["avg_similarity"]
                all_evaluations[config_name]["query_count"] += 1
                all_evaluations[config_name]["description"] = eval_result["description"]
                if config_name == best_config[0]:
                    all_evaluations[config_name]["best_match_count"] += 1

            # æ˜¾ç¤ºæœ€ä½³é…ç½®çš„æ£€ç´¢ç»“æœ
            print(f"\n   æœ€ä½³é…ç½®æ£€ç´¢ç»“æœ ({best_config[0]}):")
            for j, doc in enumerate(results[best_config[0]]["documents"][:2], 1):
                print(f"   [{j}] {doc[:60]}...")

        return all_evaluations

    def print_summary(self, evaluations: Dict):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print()
        print("=" * 70)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("=" * 70)

        # è®¡ç®—å¹³å‡å€¼
        summary = []
        for config_name, eval_data in evaluations.items():
            query_count = eval_data["query_count"]
            if query_count > 0:
                avg_keyword = eval_data["total_keyword_coverage"] / query_count
                avg_similarity = eval_data["total_similarity"] / query_count
                best_count = eval_data["best_match_count"]

                # ç»¼åˆå¾—åˆ† = å…³é”®è¯è¦†ç›–(40%) + ç›¸ä¼¼åº¦(40%) + æœ€ä½³åŒ¹é…æ¬¡æ•°(20%)
                composite_score = (avg_keyword * 0.4 + avg_similarity * 0.4 +
                                   (best_count / query_count * 100) * 0.2)

                summary.append({
                    "config": config_name,
                    "description": eval_data["description"],
                    "avg_keyword": avg_keyword,
                    "avg_similarity": avg_similarity,
                    "best_count": best_count,
                    "composite_score": composite_score
                })

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        summary.sort(key=lambda x: x["composite_score"], reverse=True)

        print(f"\n{'æ’å':<4} {'é…ç½®':<28} {'å…³é”®è¯è¦†ç›–':<12} {'ç›¸ä¼¼åº¦':<12} {'æœ€ä½³æ¬¡æ•°':<10} {'ç»¼åˆå¾—åˆ†':<10}")
        print("-" * 80)

        for rank, item in enumerate(summary, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(f"{medal}{rank:<3} {item['description'][:26]:<27} "
                  f"{item['avg_keyword']:.1f}%{'':<7} "
                  f"{item['avg_similarity']:.1f}%{'':<7} "
                  f"{item['best_count']:<10} "
                  f"{item['composite_score']:.1f}")

        # æ¨èé…ç½®
        best = summary[0]
        print()
        print("=" * 70)
        print("ğŸ’¡ æ¨èé…ç½®åˆ†æ")
        print("=" * 70)
        print(f"\nğŸ† æœ€ä½³é…ç½®: {best['description']}")
        print(f"   ç»¼åˆå¾—åˆ†: {best['composite_score']:.1f}")
        print(f"   å¹³å‡å…³é”®è¯è¦†ç›–ç‡: {best['avg_keyword']:.1f}%")
        print(f"   å¹³å‡ç›¸ä¼¼åº¦: {best['avg_similarity']:.1f}%")
        print()

        # å„åœºæ™¯æ¨è
        print("ğŸ“‹ å„åœºæ™¯æ¨èï¼š")
        print("-" * 50)

        # æ‰¾å‡ºå…³é”®è¯è¦†ç›–æœ€é«˜çš„é…ç½®
        best_keyword = max(summary, key=lambda x: x["avg_keyword"])
        # æ‰¾å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„é…ç½®
        best_similarity = max(summary, key=lambda x: x["avg_similarity"])

        print(f"   â€¢ ç²¾å‡†åŒ¹é…åœºæ™¯: {best_keyword['description']}")
        print(f"     (å…³é”®è¯è¦†ç›–ç‡æœ€é«˜: {best_keyword['avg_keyword']:.1f}%)")
        print()
        print(f"   â€¢ è¯­ä¹‰ç›¸ä¼¼åœºæ™¯: {best_similarity['description']}")
        print(f"     (ç›¸ä¼¼åº¦æœ€é«˜: {best_similarity['avg_similarity']:.1f}%)")
        print()
        print(f"   â€¢ ç»¼åˆæ¨è: {best['description']}")
        print(f"     (ç»¼åˆå¾—åˆ†æœ€é«˜: {best['composite_score']:.1f})")

    def show_collection_stats(self):
        """æ˜¾ç¤º Collection ç»Ÿè®¡ä¿¡æ¯"""
        print()
        print("=" * 70)
        print("ğŸ“ˆ Collection ç»Ÿè®¡")
        print("=" * 70)

        print(f"\n{'é…ç½®':<28} {'Chunkæ•°':<10} {'å¹³å‡é•¿åº¦':<12} {'é…ç½®å‚æ•°':<20}")
        print("-" * 70)

        for name, data in self.collections.items():
            config = data["config"]
            chunks = data["chunks"]
            avg_len = sum(len(c) for c in chunks) / len(chunks)

            print(f"{config.description[:26]:<28} "
                  f"{len(chunks):<10} "
                  f"{avg_len:.1f}å­—ç¬¦{'':<5} "
                  f"size={config.chunk_size}, overlap={config.chunk_overlap}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)
            print(f"\nğŸ§¹ å·²æ¸…ç†æµ‹è¯•æ•°æ®ç›®å½•: {self.persist_directory}")


def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " åˆ†å—ç­–ç•¥æ•ˆæœå¯¹æ¯”æµ‹è¯• - BGE-large-zh + ChromaDB ".center(58) + "   â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()

    # æ£€æŸ¥ä¾èµ–
    try:
        import chromadb
        from sentence_transformers import SentenceTransformer
        from langchain_text_splitters import RecursiveCharacterTextSplitter
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("\nè¯·å®‰è£…ä¾èµ–ï¼š")
        print("pip install langchain-text-splitters chromadb sentence-transformers")
        return

    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    comparison = ChunkingComparison()

    try:
        # 1. åˆå§‹åŒ–ç¯å¢ƒ
        comparison.setup()

        # 2. è·å–æµ‹è¯•æ•°æ®
        sample_text = get_sample_documents()
        test_queries = get_test_queries()

        print(f"ğŸ“„ æµ‹è¯•æ–‡æ¡£é•¿åº¦: {len(sample_text)} å­—ç¬¦")
        print(f"â“ æµ‹è¯•æŸ¥è¯¢æ•°é‡: {len(test_queries)} ä¸ª")
        print()

        # 3. åˆ›å»º Collections
        comparison.create_collections(CHUNK_CONFIGS, sample_text)

        # 4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        comparison.show_collection_stats()

        # 5. è¿è¡Œå¯¹æ¯”æµ‹è¯•
        evaluations = comparison.run_comparison_test(test_queries)

        # 6. æ‰“å°æ€»ç»“
        comparison.print_summary(evaluations)

        print()
        print("=" * 70)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print("=" * 70)
        print()
        print("ğŸ“ ç»“è®ºè¯´æ˜ï¼š")
        print("   â€¢ å…³é”®è¯è¦†ç›–ç‡: æ£€ç´¢ç»“æœä¸­åŒ…å«æœŸæœ›å…³é”®è¯çš„æ¯”ä¾‹")
        print("   â€¢ ç›¸ä¼¼åº¦: æŸ¥è¯¢ä¸æ£€ç´¢ç»“æœçš„å‘é‡ç›¸ä¼¼åº¦")
        print("   â€¢ æœ€ä½³æ¬¡æ•°: åœ¨æ‰€æœ‰æµ‹è¯•ä¸­è·å¾—æœ€ä½³ç»“æœçš„æ¬¡æ•°")
        print("   â€¢ ç»¼åˆå¾—åˆ†: å…³é”®è¯è¦†ç›–(40%) + ç›¸ä¼¼åº¦(40%) + æœ€ä½³æ¯”ä¾‹(20%)")
        print()
        print("ğŸ”§ æ ¹æ®æµ‹è¯•ç»“æœé€‰æ‹©é€‚åˆæ‚¨åœºæ™¯çš„åˆ†å—é…ç½®ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # è¯¢é—®æ˜¯å¦æ¸…ç†
        print()
        try:
            cleanup = input("æ˜¯å¦æ¸…ç†æµ‹è¯•æ•°æ®ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
            if cleanup != 'n':
                comparison.cleanup()
        except EOFError:
            # éäº¤äº’æ¨¡å¼ä¸‹è‡ªåŠ¨æ¸…ç†
            comparison.cleanup()


if __name__ == "__main__":
    main()
